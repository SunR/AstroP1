Python 2.7.10 (default, May 23 2015, 09:44:00) [MSC v.1500 64 bit (AMD64)] on win32
Type "copyright", "credits" or "license()" for more information.
>>> ================================ RESTART ================================
>>> 
661598
crossmatchig objID
set([])
[  5.87727179e+17   5.87727227e+17   5.87727225e+17   5.87730775e+17
   5.87731186e+17]
[1237648720142401611 1237650795146510627 1237650795146445031
 1237648720142401670 1237648720142532891]
[ 146.71419105  146.74414186  146.62857334  146.63167333  146.91941573]
time at start of cross-matching =  1439597135.78
[(146.71420000000001, -1.0412999999999999), (146.7441, -0.6522), (146.62860000000001, -0.7651), (146.6317, -0.98829999999999996), (146.9194, -0.99050000000000005)]
[(0.0001, -9.6262), (0.0002, -8.7777), (0.0003, -9.0533), (0.0004, 15.5098), (0.0004, 0.0926)]
completed 0 tests, found 0 cross-matches
time =  0.742000102997
completed 1000 tests, found 74 cross-matches
time =  12.8760001659
completed 2000 tests, found 140 cross-matches
time =  25.0110001564
completed 3000 tests, found 213 cross-matches
time =  37.243999958

>>> ================================ RESTART ================================
>>> 
661598
crossmatchig objID
set([])
[  5.87727179e+17   5.87727227e+17   5.87727225e+17   5.87730775e+17
   5.87731186e+17]
[1237650762927571086 1237650762927571054 1237650372092690555
 1237650762927505634 1237650372092625185]
[ 146.71419105  146.74414186  146.62857334  146.63167333  146.91941573]
time at start of cross-matching =  1439597192.68
[(146.71420000000001, -1.0412999999999999), (146.7441, -0.6522), (146.62860000000001, -0.7651), (146.6317, -0.98829999999999996), (146.9194, -0.99050000000000005)]
[(0.0001, -9.6262), (0.0002, -8.7777), (0.0003, -9.0533), (0.0004, 15.5098), (0.0004, 0.0926)]
completed 0 tests, found 0 cross-matches
time =  0.740000009537
completed 1000 tests, found 74 cross-matches
time =  13.2400000095
completed 2000 tests, found 140 cross-matches
time =  26.1410000324
completed 3000 tests, found 213 cross-matches
time =  38.9670000076
completed 4000 tests, found 275 cross-matches
time =  51.6430001259
completed 5000 tests, found 355 cross-matches
time =  64.0290000439
completed 6000 tests, found 422 cross-matches
time =  76.1570000648
completed 7000 tests, found 492 cross-matches
time =  88.2220001221
completed 8000 tests, found 570 cross-matches
time =  100.493999958
completed 9000 tests, found 641 cross-matches
time =  112.654999971
completed 10000 tests, found 703 cross-matches
time =  124.809999943

>>> ================================ RESTART ================================
>>> 
661598
crossmatchig objID
set([])
[  5.87727179e+17   5.87727227e+17   5.87727225e+17   5.87730775e+17
   5.87731186e+17]
[1237650762927571086 1237650762927571054 1237650372092690555
 1237650762927505634 1237650372092625185]

>>> ================================ RESTART ================================
>>> 
[[  5.20000000e+01   8.85000000e-01   1.90000000e-02   0.00000000e+00
    5.80000000e-02   1.90000000e-02   1.90000000e-02   7.70000000e-02
    8.85000000e-01   7.70000000e-02   0.00000000e+00   1.00000000e+00
    0.00000000e+00]
 [  3.00000000e+01   9.33000000e-01   0.00000000e+00   3.30000000e-02
    0.00000000e+00   3.30000000e-02   0.00000000e+00   3.30000000e-02
    9.13000000e-01   5.40000000e-02   0.00000000e+00   1.00000000e+00
    0.00000000e+00]
 [  5.50000000e+01   8.18000000e-01   3.60000000e-02   0.00000000e+00
    3.60000000e-02   1.09000000e-01   0.00000000e+00   7.30000000e-02
    8.18000000e-01   7.30000000e-02   0.00000000e+00   1.00000000e+00
    0.00000000e+00]
 [  3.10000000e+01   1.00000000e+00   0.00000000e+00   0.00000000e+00
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    1.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00
    0.00000000e+00]
 [  6.70000000e+01   9.40000000e-01   0.00000000e+00   0.00000000e+00
    1.50000000e-02   4.50000000e-02   0.00000000e+00   1.50000000e-02
    9.27000000e-01   2.80000000e-02   0.00000000e+00   1.00000000e+00
    0.00000000e+00]
 [  3.20000000e+01   8.12000000e-01   0.00000000e+00   0.00000000e+00
    9.40000000e-02   9.40000000e-02   0.00000000e+00   9.40000000e-02
    7.16000000e-01   1.80000000e-01   0.00000000e+00   1.00000000e+00
    0.00000000e+00]
 [  2.80000000e+01   8.21000000e-01   0.00000000e+00   3.60000000e-02
    3.60000000e-02   7.10000000e-02   3.60000000e-02   7.10000000e-02
    8.19000000e-01   7.40000000e-02   0.00000000e+00   1.00000000e+00
    0.00000000e+00]]
62190 190225 415529 667944
500 500
Number of galaxies studied: 667944
('ra', 'dec', 'mjd', 'plate', 'fiberID', 'z', 'zErr', 'rChi2', 'velDisp', 'velDispErr', 'extinction_r', 'petroMag_r', 'psfMag_r', 'psfMagErr_r', 'modelMag_u', 'modelMagErr_u', 'modelMag_g', 'modelMagErr_g', 'modelMag_r', 'modelMagErr_r', 'modelMag_i', 'modelMagErr_i', 'modelMag_z', 'modelMagErr_z', 'petroR50_r', 'petroR90_r', 'nii_6584_flux', 'nii_6584_flux_err', 'h_alpha_flux', 'h_alpha_flux_err', 'oiii_5007_flux', 'oiii_5007_flux_err', 'h_beta_flux', 'h_beta_flux_err', 'h_delta_flux', 'h_delta_flux_err', 'd4000', 'd4000_err', 'bptclass', 'lgm_tot_p50', 'sfr_tot_p50', 'objID', 'specObjID')
Number of attributes for each galaxy: 43
Number of galaxies included: 661598
>>> print data['objID'][:5]

Traceback (most recent call last):
  File "<pyshell#0>", line 1, in <module>
    print data['objID'][:5]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
>>> o = data['objID']

Traceback (most recent call last):
  File "<pyshell#1>", line 1, in <module>
    o = data['objID']
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
>>> print data2['objID'][:5]
[1237648720142401611 1237650795146510627 1237650795146445031
 1237648720142401670 1237648720142532891]
>>> print data2['specObjID'][:5]
[299489677444933632 299489952322840576 299490502078654464
 299491051834468352 299491326712375296]
>>> ================================ RESTART ================================
>>> 
('ra', 'dec', 'mjd', 'plate', 'fiberID', 'z', 'zErr', 'rChi2', 'velDisp', 'velDispErr', 'extinction_r', 'petroMag_r', 'psfMag_r', 'psfMagErr_r', 'modelMag_u', 'modelMagErr_u', 'modelMag_g', 'modelMagErr_g', 'modelMag_r', 'modelMagErr_r', 'modelMag_i', 'modelMagErr_i', 'modelMag_z', 'modelMagErr_z', 'petroR50_r', 'petroR90_r', 'nii_6584_flux', 'nii_6584_flux_err', 'h_alpha_flux', 'h_alpha_flux_err', 'oiii_5007_flux', 'oiii_5007_flux_err', 'h_beta_flux', 'h_beta_flux_err', 'h_delta_flux', 'h_delta_flux_err', 'd4000', 'd4000_err', 'bptclass', 'lgm_tot_p50', 'sfr_tot_p50', 'objID', 'specObjID')
(1000L, 9L)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=3,
  shrinking=True, tol=0.001, verbose=False)
Training accuracy =  0.026
Testing accuracy =  0.002
>>> ================================ RESTART ================================
>>> 
(1000L, 9L)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=3,
  shrinking=True, tol=0.001, verbose=False)
Training accuracy =  0.288
Testing accuracy =  0.024
>>> ================================ RESTART ================================
>>> 
(1000L, 9L)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=3,
  shrinking=True, tol=0.001, verbose=False)
Training accuracy =  0.288
Testing accuracy =  0.024
>>> ================================ RESTART ================================
>>> 
(1000L, 9L)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Training accuracy =  0.572
Testing accuracy =  0.986
>>> ================================ RESTART ================================
>>> 
(1000L, 9L)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Training accuracy =  0.184
Testing accuracy =  0.004
>>> ================================ RESTART ================================
>>> 
(1000L, 9L)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Training accuracy =  0.572
Testing accuracy =  0.986
>>> ================================ RESTART ================================
>>> 
(10000L, 6L)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)

>>> ================================ RESTART ================================
>>> 
(10000L, 5L)
Time before training = 
>>> ================================ RESTART ================================
>>> 
(1000L, 5L)
Time before training =  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  1.38300013542 seconds

>>> ================================ RESTART ================================
>>> 
(1000L, 5L)
Time before training =  1439628774.12
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  1.36500000954 seconds
Training accuracy =  0.322
Time =  4.80200004578 seconds

Testing accuracy =  0.262
Time =  8.22199988365 seconds
>>> ================================ RESTART ================================
>>> 
(1000L, 5L)
Time before training =  1439628799.76
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  1.70299983025 seconds
Training accuracy =  0.0
Time =  5.9240000248 seconds

Testing accuracy =  0.0
Time =  10.2309999466 seconds
>>> ================================ RESTART ================================
>>> 
(1000L, 5L)
Time before training =  1439628824.02
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  1.63700008392 seconds
Training accuracy =  0.0
Time =  5.9430000782 seconds

Testing accuracy =  0.0
Time =  10.1809999943 seconds
>>> ================================ RESTART ================================
>>> 
(1000L, 5L)
Time before training =  1439628848.71
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  1.7009999752 seconds
Training accuracy =  0.0
Time =  5.90300011635 seconds

Testing accuracy =  0.0
Time =  10.1019999981 seconds
>>> ================================ RESTART ================================
>>> 
[ 19.20599  17.04256  15.99127  15.51565  15.12117]
(10000L, 5L)
Time before training =  1439628920.84
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  160.370999813 seconds

>>> ================================ RESTART ================================
>>> 
[ 19.20599  17.04256  15.99127  15.51565  15.12117]
(10000L, 5L)
Time before training =  1439629306.79
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  170.871999979 seconds
At iteration 0 Training accuracy = 

Traceback (most recent call last):
  File "C:/Users/S/Documents/Sonu/SciComp/Research/AstroP1/ap1_sklearn_svm1.py", line 59, in <module>
    print "At iteration", i, "Training accuracy = ", numCorrect/i
ZeroDivisionError: division by zero
>>> ================================ RESTART ================================
>>> 
[ 19.20599  17.04256  15.99127  15.51565  15.12117]
(10000L, 5L)
Time before training =  1439630986.23
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  186.630000114 seconds
At iteration 100 Training accuracy =  0.01
At iteration 200 Training accuracy =  0.005
At iteration 300 Training accuracy =  0.00333333333333
At iteration 400 Training accuracy =  0.0025
At iteration 500 Training accuracy =  0.002
At iteration 600 Training accuracy =  0.00166666666667
At iteration 700 Training accuracy =  0.00142857142857
At iteration 800 Training accuracy =  0.00125

>>> ================================ RESTART ================================
>>> 
[ 19.20599  17.04256  15.99127  15.51565  15.12117]
(10000L, 5L)
Time before training =  1439632406.14
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  165.667999983 seconds
[ 19.20599  17.04256  15.99127  15.51565  15.12117]
[ 1.]
[ 18.64089  17.28883  16.49727  16.04799  15.72262]
[  1.69277886e-315]
[ 18.55686  16.46329  15.35632  14.88631  14.47813]
[  3.17261523e-315]
[ 20.36567  18.26141  17.13288  16.6342   16.20322]
[  3.17261511e-315]
[ 19.23516  17.30615  16.19294  15.7707   15.38741]
[  3.17261500e-315]
[ 21.69363  19.51635  17.88627  17.27815  16.90043]
[  3.17261488e-315]
[ 17.85308  15.79116  14.82322  14.34608  13.87637]
[  3.17261476e-315]
[ 20.58667  18.68907  17.80396  17.39398  17.03654]
[  3.17261464e-315]
[ 17.87896  15.98305  15.09698  14.65252  14.32997]
[  3.17261452e-315]
[ 19.12393  17.16855  16.18201  15.73156  15.35614]
[  6.21619404e-316]
[ 21.83866  19.61908  18.07494  17.48474  17.0399 ]
[  6.21619285e-316]
[ 20.19409  18.56988  17.76433  17.39452  17.05394]
[  6.21619167e-316]
[ 18.92397  16.84635  15.82429  15.36749  14.98241]
[  6.21619048e-316]
[ 18.11086  16.1827   15.3071   14.8465   14.50426]
[  6.21618929e-316]
[ 20.7053   18.81659  17.76706  17.29209  16.92201]
[  6.21618811e-316]
[ 18.39571  16.53355  15.64989  15.21479  14.86047]
[  6.21618692e-316]
[ 20.12062  18.28579  17.06535  16.54475  16.17289]
[  6.21618574e-316]
[ 18.27211  16.35393  15.4992   15.07878  14.74794]
[  6.21618455e-316]
[ 19.8211   17.68453  16.52719  16.05909  15.69338]
[  6.21618336e-316]
[ 20.71924  18.95131  17.3896   16.86432  16.50899]
[  6.21618218e-316]
[ 21.45172  19.29389  17.52462  16.93133  16.59223]
[  6.21618099e-316]
[ 21.70982  19.06435  17.66794  17.11675  16.73755]
[  6.21617981e-316]
[ 19.52944  17.38794  16.26929  15.8301   15.46233]
[  6.21617862e-316]
[ 19.08624  16.97029  15.84682  15.39137  15.0143 ]
[  6.21617744e-316]
[ 20.6768   18.58024  17.41307  16.9583   16.56925]
[  6.21617625e-316]
[ 19.82912  17.60688  16.36561  15.86783  15.48853]
[  6.21617506e-316]
[ 20.42048  18.42491  17.22064  16.76864  16.43711]
[  6.21617388e-316]
[ 20.71007  18.48612  17.35504  16.88805  16.55814]
[  6.21607190e-316]
[ 18.71495  16.7991   15.9363   15.52322  15.23778]
[  6.21607072e-316]
[ 20.58724  18.60419  17.27333  16.74672  16.41787]
[  6.21606953e-316]
[ 21.11977  19.03857  17.63189  17.1124   16.77008]
[  6.21606835e-316]
[ 20.46502  18.40669  17.17135  16.6808   16.32031]
[  6.21606716e-316]
[ 20.53538  18.68714  17.57839  17.13948  16.80651]
[  6.21606597e-316]
[ 19.34803  17.4276   16.36593  15.94021  15.5956 ]
[  6.21606479e-316]
[ 20.9524   18.76354  17.20914  16.63882  16.29662]
[  6.21606360e-316]
[ 21.2614   18.99883  17.71458  17.23038  16.91018]
[  6.21606242e-316]
[ 19.78329  17.71956  16.58833  16.12964  15.75646]
[  6.21606123e-316]
[ 19.33034  17.32969  16.1939   15.72212  15.34574]
[  6.21606005e-316]
[ 20.61796  18.58474  17.61328  17.18873  16.89646]
[  6.21605886e-316]
[ 20.52672  18.46244  17.13189  16.63376  16.29519]
[  6.21605767e-316]
[ 21.28914  18.65491  17.18333  16.67805  16.31177]
[  6.21605649e-316]
[ 20.25864  18.19353  17.18458  16.74544  16.38326]
[  6.21605530e-316]
[ 19.60723  17.56247  16.52196  16.08775  15.72697]
[  6.21605412e-316]
[ 20.14179  18.14035  17.05786  16.60759  16.25116]
[  6.21605293e-316]
[ 18.60115  16.66619  15.7559   15.35125  15.04916]
[  6.21605175e-316]
[ 20.68657  18.61404  17.43984  16.95275  16.59838]
[  6.21605056e-316]
[ 19.84797  17.80748  16.70496  16.25117  15.90555]
[  6.21604937e-316]
[ 18.81027  16.86746  15.92351  15.5018   15.1683 ]
[  6.21604819e-316]
[ 18.16863  16.29194  15.35627  14.9017   14.59303]
[  6.21604700e-316]
[ 21.02748  18.81518  17.71993  17.23988  16.8748 ]
[  6.21604582e-316]
[ 21.14625  18.55646  17.30849  16.8147   16.48435]
[  6.21604463e-316]
[ 17.9303   16.16564  15.39575  15.01975  14.7225 ]
[  6.21604345e-316]
[ 17.73902  15.76031  14.8168   14.39204  14.05096]
[  6.21604226e-316]
[ 21.24175  19.16072  17.7042   17.19713  16.80668]
[  6.21604107e-316]
[ 19.84369  17.88301  16.68527  16.24076  15.87511]
[  6.21603989e-316]
[ 20.78371  18.57651  17.20654  16.68848  16.29751]
[  6.21603870e-316]
[ 18.28523  16.65745  15.89487  15.52012  15.28212]
[  6.21603752e-316]
[ 21.08832  18.9261   17.39112  16.87301  16.47473]
[  6.21603633e-316]
[ 21.00337  18.7336   17.30049  16.76358  16.40823]
[  6.21603515e-316]
[ 20.05569  17.94987  16.80665  16.33913  15.9473 ]
[  6.21603396e-316]
[ 19.92138  18.03531  17.1092   16.69703  16.35272]
[  6.21603277e-316]
[ 19.98236  18.01241  16.94067  16.48891  16.19028]
[  6.21603159e-316]
[ 18.38651  16.41023  15.44206  15.01857  14.66557]
[  6.21603040e-316]
[ 20.61785  18.4624   17.44658  17.02888  16.62807]
[  6.21602922e-316]
[ 19.86577  18.12805  16.89802  16.39474  16.04404]
[  6.21602803e-316]
[ 18.15663  16.19928  15.25982  14.81814  14.48767]
[  6.21602684e-316]
[ 17.9901   15.93916  14.96368  14.52592  14.1624 ]
[  6.21602566e-316]
[ 18.25999  16.33459  15.34496  14.9227   14.56517]
[  6.21602447e-316]
[ 15.93876  14.0377   13.12335  12.66916  12.32974]
[  6.21567349e-316]
[ 19.34419  17.90877  16.92174  16.46033  16.09106]
[  6.21567230e-316]
[ 19.85204  17.76121  16.64526  16.1724   15.78838]
[  6.21567112e-316]
[ 21.72499  19.22846  17.79082  17.26768  16.87781]
[  6.21566993e-316]
[ 21.325    19.04108  17.71834  17.20456  16.87299]
[  6.21566875e-316]
[ 21.94552  19.24689  17.65541  17.09941  16.72058]
[  6.21566756e-316]
[ 18.42458  16.49588  15.47034  15.03816  14.6836 ]
[  6.21566637e-316]
[ 22.79258  19.2855   17.705    17.13461  16.77739]
[  6.21566519e-316]
[ 18.35453  16.33981  15.39312  14.95005  14.60134]
[  6.21566400e-316]
[ 21.07608  18.7842   17.32569  16.76527  16.44823]
[  6.21566282e-316]
[ 19.0768   17.34698  16.31064  15.82152  15.42542]
[  6.21566163e-316]
[ 19.762    17.55605  16.35719  15.88287  15.51697]
[  6.21566045e-316]
[ 20.40014  18.46184  17.10792  16.62078  16.25373]
[  6.21565926e-316]
[ 18.95097  17.02529  16.06443  15.63728  15.2926 ]
[  6.21565807e-316]
[ 20.40005  18.41722  17.28064  16.82035  16.40719]
[  6.21565689e-316]
[ 20.89042  18.93307  17.69135  17.24528  16.89722]
[  6.21565570e-316]
[ 19.86844  17.53795  16.37504  15.88932  15.52141]
[  6.21565452e-316]
[ 20.36008  18.40693  16.99627  16.46618  16.09937]
[  6.21565333e-316]
[ 21.04448  18.98621  17.59736  17.09439  16.70999]
[  6.21565215e-316]
[ 21.19566  19.02937  17.51925  16.9688   16.61288]
[  6.21565096e-316]
[ 18.9351   16.90634  15.78275  15.32152  14.95001]
[  6.21564977e-316]
[ 19.40366  17.56037  16.57615  16.14394  15.78645]
[  6.21564859e-316]
[ 19.68751  17.86257  16.79719  16.37099  15.99198]
[  6.21564740e-316]
[ 17.85831  15.84691  14.93577  14.49827  14.13597]
[  6.21564622e-316]
[ 19.01295  18.18083  17.42038  16.94194  16.71736]
[  6.21564503e-316]
[ 20.22564  18.28892  17.33993  16.93011  16.58439]
[  6.21564385e-316]
[ 20.40842  18.37974  17.07822  16.56254  16.17824]
[  6.21564266e-316]
[ 20.87057  18.70977  17.51389  17.02421  16.66261]
[  6.21564147e-316]
[ 19.93884  18.4808   17.51237  17.03146  16.68892]
[  6.21564029e-316]
[ 18.74735  16.76196  15.65525  15.1408   14.72539]
[  6.21563910e-316]
[ 16.90097  14.93122  14.05898  13.6262   13.30915]
[  6.21563792e-316]
[ 20.04411  18.0092   16.85587  16.35334  15.99156]
[  6.21563673e-316]
[ 18.35726  17.04508  16.26218  15.85453  15.55852]
[  6.21563554e-316]
At iteration 100 Training accuracy =  0.01
[ 18.01215  16.12752  15.22819  14.81207  14.44211]
[  6.21563436e-316]
[ 20.22522  18.36343  17.63115  17.12362  16.86603]
[  6.21563317e-316]
[ 19.77174  17.80984  16.75384  16.28047  15.92058]
[  6.21563199e-316]
[ 18.58105  16.6793   15.75966  15.32102  14.99098]
[  6.21563080e-316]
[ 18.78711  16.81365  15.87321  15.43314  15.09687]
[  6.21562962e-316]
[ 20.03855  18.00414  16.94333  16.48262  16.10795]
[  6.21562843e-316]
[ 18.88325  16.89315  15.89673  15.46113  15.0855 ]
[  6.21562724e-316]
[ 21.40298  18.91438  17.48961  16.96956  16.57647]
[  6.21562606e-316]
[ 21.2439   19.15327  17.62254  17.03922  16.7229 ]
[  6.21572329e-316]
[ 21.05532  19.0601   17.7803   17.26405  16.91148]
[  6.21572211e-316]
[ 19.96121  18.80157  17.59464  17.10621  16.72363]
[  6.21572092e-316]
[ 20.60951  18.23057  16.85562  16.35337  16.01028]
[  6.21571973e-316]
[ 20.27873  18.35784  17.11868  16.60085  16.19558]
[  6.21571855e-316]
[ 19.59714  17.47134  16.30515  15.82144  15.44798]
[  6.21571736e-316]
[ 21.11573  18.97931  17.52197  16.95778  16.58451]
[  6.21571618e-316]
[ 18.25705  16.31657  15.41997  14.99847  14.68743]
[  6.21571499e-316]
[ 18.28124  16.23312  15.25505  14.81687  14.45675]
[  6.21571380e-316]
[ 21.49414  19.57605  17.91624  17.31851  16.9146 ]
[  6.21571262e-316]
[ 21.19102  18.9569   17.7337   17.2223   16.8476 ]
[  6.21571143e-316]
[ 18.99062  17.07226  16.077    15.64486  15.30027]
[  6.21571025e-316]
[ 21.19349  19.34463  17.87124  17.30133  16.8841 ]
[  6.21570906e-316]
[ 19.78147  17.63792  16.46951  15.99795  15.62879]
[  6.21570788e-316]
[ 20.38953  18.33914  17.21253  16.73705  16.34846]
[  6.21570669e-316]
[ 18.57407  16.21185  15.08886  14.6267   14.22603]
[  6.21570550e-316]
[ 20.80762  18.81673  17.78981  17.34866  17.00762]
[  6.21570432e-316]
[ 19.47111  17.30832  16.22974  15.76782  15.37523]
[  6.21570313e-316]
[ 19.78281  17.79661  16.74375  16.2786   15.92705]
[  6.21570195e-316]
[ 17.53626  16.10129  15.29478  14.86786  14.52887]
[  6.21570076e-316]
[ 20.97117  18.51388  17.34535  16.78072  16.3408 ]
[  6.21569958e-316]
[ 18.06775  15.87725  14.79468  14.29916  13.89613]
[  6.21569839e-316]
[ 20.17109  18.31542  17.19308  16.63629  16.24751]
[  6.21569720e-316]
[ 19.8875   18.03511  16.80489  16.29148  15.85893]
[  6.21569602e-316]
[ 18.14591  16.13817  15.0766   14.57298  14.1823 ]
[  6.21569483e-316]
[ 19.52552  17.53687  16.52507  16.08805  15.72934]
[  6.21569365e-316]
[ 21.39738  19.34403  17.85334  17.31801  16.93941]
[  6.21569246e-316]
[ 19.42016  17.37621  16.26271  15.75622  15.36211]
[  6.21569128e-316]
[ 19.16857  17.17731  16.18544  15.73481  15.36201]
[  6.21569009e-316]
[ 19.12572  17.16293  16.25484  15.81296  15.50886]
[  6.21568890e-316]
[ 17.64082  15.8659   15.0013   14.58122  14.22377]
[  6.21568772e-316]
[ 20.92007  18.91019  17.69565  17.22462  16.84791]
[  6.21568653e-316]
[ 20.17492  18.0108   16.85334  16.38405  16.02318]
[  6.21568535e-316]
[ 20.27802  18.3914   17.33298  16.87294  16.49665]
[  6.21568416e-316]
[ 19.88691  17.84604  16.70034  16.22763  15.83263]
[  6.21568298e-316]
[ 19.84698  17.80694  16.59866  16.1361   15.75601]
[  6.21568179e-316]
[ 19.78606  17.83212  16.66105  16.20053  15.83866]
[  6.21568060e-316]
[ 19.85405  17.68857  16.47571  16.03102  15.66103]
[  6.21567942e-316]
[ 19.84158  17.53374  16.33795  15.86926  15.49918]
[  6.21567823e-316]
[ 19.72589  17.67456  16.5237   16.06508  15.71526]
[  6.21567705e-316]
[ 19.69693  17.54342  16.37163  15.87983  15.52692]
[  6.21567586e-316]
[ 19.71769  17.819    16.80257  16.39466  16.05018]
[  6.21577309e-316]
[ 19.80354  17.68052  16.54697  16.10391  15.77316]
[  6.21577191e-316]
[ 18.96965  16.97965  16.0684   15.63433  15.28153]
[  6.21577072e-316]
[ 18.95276  16.96692  16.06065  15.62897  15.28367]
[  6.21576954e-316]
[ 19.92387  18.0295   16.81124  16.33128  15.96304]
[  6.21576835e-316]
[ 17.39551  15.5295   14.65254  14.24767  13.90386]
[  6.21576716e-316]
[ 19.37221  17.46229  16.35175  15.90153  15.51745]
[  6.21576598e-316]
[ 20.999    18.89268  17.42349  16.89059  16.55884]
[  6.21576479e-316]
[ 19.447    17.3877   16.33401  15.90024  15.54232]
[  6.21576361e-316]
[ 19.79361  17.81226  16.82915  16.40892  16.04888]
[  6.21576242e-316]
[ 18.12166  16.13347  15.16276  14.75319  14.39996]
[  6.21576124e-316]
[ 19.60759  17.45659  16.35392  15.90154  15.54599]
[  6.21576005e-316]
[ 19.98561  17.99569  16.80353  16.3272   15.96951]
[  6.21575886e-316]
[ 20.05402  18.31755  17.14545  16.66729  16.36257]
[  6.21575768e-316]
[ 21.06896  19.25765  17.67744  17.06896  16.70802]
[  6.21575649e-316]
[ 18.31678  16.34192  15.34133  14.91031  14.66926]
[  6.21575531e-316]
[ 21.35115  18.87553  17.49045  16.97206  16.55308]
[  6.21575412e-316]
[ 19.09982  17.30977  16.35467  15.97244  15.64657]
[  6.21575293e-316]
[ 19.89879  17.96495  16.88321  16.43847  16.09317]
[  6.21575175e-316]
[ 18.56525  16.38824  15.36853  14.89313  14.51602]
[  6.21575056e-316]
[ 19.46568  18.5845   17.37051  16.8219   16.53102]
[  6.21574938e-316]
[ 18.13085  16.1489   15.21012  14.80094  14.46621]
[  6.21574819e-316]
[ 20.77316  18.67514  17.29156  16.78384  16.44692]
[  6.21574701e-316]
[ 21.39954  18.96022  17.54685  17.03154  16.65552]
[  6.21574582e-316]
[ 18.71804  16.75865  15.86703  15.4534   15.13942]
[  6.21574463e-316]
[ 18.98793  17.00207  16.0575   15.63976  15.29508]
[  6.21574345e-316]
[ 20.34493  18.229    16.97006  16.47754  16.13929]
[  6.21574226e-316]
[ 21.08531  19.0113   17.67698  17.1771   16.80449]
[  6.21574108e-316]
[ 22.00359  19.24593  17.70964  17.14687  16.82157]
[  6.21573989e-316]
[ 20.96732  18.93924  17.52175  17.00281  16.66547]
[  6.21573871e-316]
[ 19.61249  17.44455  16.31778  15.86383  15.51755]
[  6.21573752e-316]
[ 19.39609  17.30155  16.17189  15.68841  15.29636]
[  6.21573633e-316]
[ 20.52969  18.51693  17.38744  16.93753  16.54018]
[  6.21573515e-316]
[ 19.95778  18.00365  17.07836  16.66547  16.33943]
[  6.21573396e-316]
[ 17.86083  15.91709  14.93625  14.5069   14.17768]
[  6.21573278e-316]
[ 19.31905  17.36956  16.31326  15.88333  15.49345]
[  6.21573159e-316]
[ 20.66804  18.87217  17.51571  17.01532  16.67326]
[  6.21573041e-316]
[ 21.59823  19.08348  17.61368  17.09137  16.75341]
[  6.21572922e-316]
[ 20.52987  18.51738  17.41538  16.96401  16.61951]
[  6.21572803e-316]
[ 18.70901  16.88343  16.02844  15.63686  15.32555]
[  6.21572685e-316]
[ 19.36005  17.48323  16.41125  15.99306  15.63315]
[  6.21572566e-316]
[ 20.63016  18.84986  17.58931  17.08916  16.73033]
[  6.21592250e-316]
[ 18.0411   16.03914  15.12405  14.67963  14.34461]
[  6.21592131e-316]
[ 18.18362  16.62407  15.77284  15.33367  15.01876]
[  6.21592013e-316]
[ 19.93526  17.93137  16.86361  16.42258  16.06202]
[  6.21591894e-316]
[ 17.73866  15.90782  15.07428  14.6807   14.385  ]
[  6.21591776e-316]
[ 20.04984  18.41789  17.33097  16.83904  16.49349]
[  6.21591657e-316]
[ 18.23898  16.25538  15.28834  14.85465  14.48896]
[  6.21591538e-316]
[ 20.19154  18.17954  17.06876  16.60996  16.21451]
[  6.21591420e-316]
[ 19.39762  17.25001  16.11166  15.64407  15.25669]
[  6.21591301e-316]
[ 19.85218  17.92705  16.84403  16.38481  16.04532]
[  6.21591183e-316]
At iteration 200 Training accuracy =  0.005
[ 19.27333  17.18656  15.99403  15.48859  15.08199]
[  6.21591064e-316]
[ 20.54299  18.51421  17.25453  16.75063  16.39768]
[  6.21590945e-316]
[ 20.56245  18.81782  17.62605  17.16138  16.83057]
[  6.21590827e-316]
[ 20.90162  18.90805  17.65707  17.13751  16.85701]
[  6.21590708e-316]
[ 20.22493  17.91425  16.62671  16.13508  15.77368]
[  6.21590590e-316]
[ 19.26941  17.19986  16.08668  15.63639  15.28553]
[  6.21590471e-316]
[ 21.23747  18.96373  17.47466  16.93475  16.62042]
[  6.21590353e-316]
[ 19.90508  17.67382  16.40211  15.93747  15.52523]
[  6.21590234e-316]
[ 20.37932  18.42486  17.01996  16.51672  16.15936]
[  6.21590115e-316]
[ 20.59694  18.56581  17.21559  16.70033  16.34952]
[  6.21589997e-316]
[ 20.5576   18.23067  16.97865  16.47528  16.08078]
[  6.21589878e-316]
[ 19.20842  17.19314  16.12263  15.64179  15.2266 ]
[  6.21589760e-316]
[ 19.35981  17.10647  16.00574  15.54355  15.16669]
[  6.21589641e-316]
[ 19.80669  17.74221  16.64622  16.18434  15.80552]
[  6.21589523e-316]
[ 19.22408  17.83669  16.96983  16.56235  16.28738]
[  6.21589404e-316]
[ 19.87826  17.62331  16.36165  15.87033  15.50819]
[  6.21589285e-316]
[ 17.32362  15.45352  14.59809  14.20171  13.88125]
[  6.21589167e-316]
[ 18.32162  16.30784  15.33865  14.88619  14.53821]
[  6.21589048e-316]
[ 20.91569  18.97351  17.46319  16.92348  16.56973]
[  6.21588930e-316]
[ 18.04727  16.13908  15.24295  14.8138   14.49665]
[  6.21588811e-316]
[ 21.12566  18.72733  17.27458  16.73938  16.38838]
[  6.21588693e-316]
[ 20.53325  18.42505  17.45081  17.01371  16.6513 ]
[  6.21588574e-316]
[ 20.38512  18.12997  17.02271  16.56377  16.18731]
[  6.21588455e-316]
[ 19.15124  17.07844  16.0981   15.66134  15.30928]
[  6.21588337e-316]
[ 18.82862  16.87884  15.90911  15.47808  15.12864]
[  6.21588218e-316]
[ 21.60149  19.0227   17.48196  16.91469  16.56665]
[  6.21588100e-316]
[ 20.89013  18.90401  17.84315  17.38242  17.009  ]
[  6.21587981e-316]
[ 18.11313  16.17303  15.26269  14.85856  14.53757]
[  6.21587863e-316]
[ 19.79594  17.8462   16.72505  16.28117  15.92722]
[  6.21587744e-316]
[ 20.17327  18.27579  17.3126   16.86305  16.50756]
[  6.21587625e-316]
[ 20.8979   18.80631  17.42864  16.90684  16.54169]
[  6.21587507e-316]
[ 21.52413  19.2693   17.77417  17.24927  16.85465]
[  6.21617151e-316]
[ 18.16406  16.20661  15.27874  14.83903  14.48245]
[  6.21617032e-316]
[ 19.95827  18.12416  17.13576  16.70655  16.34585]
[  6.21616914e-316]
[ 18.64384  16.4615   15.53028  15.10669  14.73519]
[  6.21616795e-316]
[ 19.93144  17.71418  16.59238  16.11721  15.72224]
[  6.21616676e-316]
[ 17.33712  15.98737  15.32055  14.9729   14.70855]
[  6.21616558e-316]
[ 21.13325  19.08819  17.69866  17.16971  16.75109]
[  6.21616439e-316]
[ 20.01858  18.04152  16.99685  16.53262  16.14141]
[  6.21616321e-316]
[ 20.75952  18.5713   17.12123  16.56575  16.19161]
[  6.21616202e-316]
[ 18.1227   16.049    14.98545  14.4886   14.11412]
[  6.21616084e-316]
[ 19.03749  17.02909  16.00492  15.53115  15.16114]
[  6.21615965e-316]
[ 20.03407  18.00834  16.71066  16.19694  15.77152]
[  6.21615846e-316]
[ 19.0322   16.73444  15.71799  15.2498   14.85997]
[  6.21615728e-316]
[ 21.08019  18.98957  17.80855  17.26017  16.84026]
[  6.21615609e-316]
[ 19.83992  17.88029  16.90446  16.43933  16.06312]
[  6.21615491e-316]
[ 20.00396  17.95877  16.69763  16.20144  15.84225]
[  6.21615372e-316]
[ 20.12577  18.28646  17.02255  16.54673  16.16935]
[  6.21615254e-316]
[ 18.50414  16.61119  15.72121  15.29064  14.94693]
[  6.21615135e-316]
[ 20.1143   18.24737  17.07004  16.59828  16.23434]
[  6.21615016e-316]
[ 19.44712  17.57886  16.42723  15.93614  15.57297]
[  6.21614898e-316]
[ 20.28568  18.40926  17.3114   16.89348  16.63133]
[  6.21614779e-316]
[ 19.43245  17.53476  16.33934  15.85115  15.48956]
[  6.21614661e-316]
[ 20.45278  18.67073  17.46198  16.97625  16.6796 ]
[  6.21614542e-316]
[ 20.41892  18.19101  16.94112  16.46051  16.10096]
[  6.21614423e-316]
[ 21.04264  18.74373  17.23388  16.66676  16.32863]
[  6.21614305e-316]
[ 19.40867  17.45798  16.35077  15.89868  15.51391]
[  6.21614186e-316]
[ 19.97482  18.02121  16.85057  16.36889  16.05547]
[  6.21614068e-316]
[ 17.66782  15.83319  14.981    14.61208  14.31982]
[  6.21613949e-316]
[ 20.79791  19.08969  17.7067   17.20585  16.87699]
[  6.21613831e-316]
[ 19.13319  17.25182  16.35184  15.93894  15.6089 ]
[  6.21613712e-316]
[ 20.31127  18.89581  17.84746  17.37625  17.06101]
[  6.21613593e-316]
[ 18.78759  16.67648  15.68419  15.24007  14.88724]
[  6.21613475e-316]
[ 17.82407  15.9369   15.06752  14.65287  14.34046]
[  6.21613356e-316]
[ 20.77586  18.96925  17.62986  17.05699  16.65469]
[  6.21613238e-316]
[ 18.24526  16.3983   15.57267  15.18067  14.87555]
[  6.21613119e-316]
[ 18.54833  16.5895   15.555    15.13713  14.77879]
[  6.21613001e-316]
[ 20.10595  18.20132  17.22983  16.83064  16.48548]
[  6.21612882e-316]
[ 20.49296  18.41169  16.98082  16.45356  16.10879]
[  6.21612763e-316]
[ 18.28203  16.51557  15.68848  15.29625  14.99518]
[  6.21612645e-316]
[ 20.88599  19.21426  17.71666  17.19784  16.83872]
[  6.21612526e-316]
[ 19.26691  17.55622  16.48687  16.01806  15.67296]
[  6.21612408e-316]
[ 20.90369  18.71795  17.33156  16.80167  16.43473]
[  6.21632091e-316]
[ 20.70559  18.39092  17.07383  16.57286  16.23001]
[  6.21631973e-316]
[ 21.03149  19.07707  17.73715  17.2336   16.8617 ]
[  6.21631854e-316]
[ 19.26046  17.29502  16.34898  15.92885  15.60547]
[  6.21631736e-316]
[ 19.65712  17.62264  16.57362  16.10565  15.75972]
[  6.21631617e-316]
[ 19.02789  17.00826  16.04581  15.60794  15.27542]
[  6.21631498e-316]
[ 21.59885  19.31317  17.80662  17.27722  16.92658]
[  6.21631380e-316]
[ 19.73685  17.91863  17.03149  16.61835  16.31727]
[  6.21631261e-316]
[ 20.30649  18.38458  17.13557  16.65814  16.30147]
[  6.21631143e-316]
[ 20.46593  18.4214   17.29201  16.82745  16.4967 ]
[  6.21631024e-316]
[ 21.06483  19.16387  17.84842  17.36627  17.05714]
[  6.21630906e-316]
[ 18.42268  16.51945  15.45203  14.95661  14.60723]
[  6.21630787e-316]
[ 18.54737  16.66773  15.74516  15.31888  14.99036]
[  6.21630668e-316]
[ 20.62175  18.79346  17.36775  16.85389  16.52359]
[  6.21630550e-316]
[ 19.94348  18.35132  17.27091  16.81533  16.49537]
[  6.21630431e-316]
[ 21.2149   19.03733  17.80622  17.31303  16.96405]
[  6.21630313e-316]
[ 20.59527  18.8546   17.62803  17.1358   16.80075]
[  6.21630194e-316]
[ 20.3145   18.22919  17.00813  16.52914  16.16896]
[  6.21630075e-316]
[ 20.49579  18.84944  17.77466  17.31811  17.03784]
[  6.21629957e-316]
[ 19.14768  17.2085   16.12156  15.64272  15.29503]
[  6.21629838e-316]
[ 20.40039  18.35192  17.05488  16.56599  16.22522]
[  6.21629720e-316]
[ 20.73387  18.34452  17.11262  16.63076  16.26917]
[  6.21629601e-316]
[ 19.41109  17.4781   16.41883  15.98692  15.61273]
[  6.21629483e-316]
[ 21.21528  18.85696  17.4374   16.94406  16.6509 ]
[  6.21629364e-316]
[ 19.23676  17.07106  15.96931  15.49861  15.11827]
[  6.21629245e-316]
[ 18.16106  16.23617  15.35421  14.91572  14.58673]
[  6.21629127e-316]
[ 19.92378  18.07269  16.98015  16.50656  16.18322]
[  6.21629008e-316]
[ 19.85584  17.8641   16.63666  16.17131  15.81093]
[  6.21628890e-316]
At iteration 300 Training accuracy =  0.00333333333333
[ 21.89744  19.13887  17.43559  16.84752  16.47454]
[  6.21628771e-316]
[ 21.7685   19.21187  17.77353  17.17236  16.83494]
[  6.21628653e-316]
[ 21.04652  18.58754  17.30639  16.82674  16.44289]
[  6.21628534e-316]
[ 17.99513  16.09512  15.25368  14.84397  14.53438]
[  6.21628415e-316]
[ 18.93444  17.04665  16.08061  15.66218  15.31771]
[  6.21628297e-316]
[ 19.80993  18.00632  17.25526  16.90495  16.67802]
[  6.21628178e-316]
[ 19.86186  17.94078  16.96648  16.58535  16.25204]
[  6.21628060e-316]
[ 19.99689  17.90181  16.61842  16.11367  15.75609]
[  6.21627941e-316]
[ 20.033    17.95294  16.87977  16.43762  16.12921]
[  6.21627823e-316]
[ 20.31546  18.04457  16.78057  16.28086  15.92059]
[  6.21627704e-316]
[ 21.87787  18.97869  17.33833  16.73087  16.37639]
[  6.21627585e-316]
[ 20.81306  18.91574  17.5976   17.06075  16.67766]
[  6.21627467e-316]
[ 19.7335   17.69869  16.47575  15.99013  15.65417]
[  6.21627348e-316]
[ 17.99165  16.02716  15.13696  14.72973  14.40056]
[  6.21557389e-316]
[ 17.45507  15.7465   14.97399  14.61254  14.34604]
[  6.21557270e-316]
[ 20.33538  18.11294  16.88645  16.41839  16.02107]
[  6.21557151e-316]
[ 20.45435  18.39202  17.08206  16.57787  16.25729]
[  6.21557033e-316]
[ 18.36079  16.42139  15.53087  15.11846  14.77365]
[  6.21556914e-316]
[ 21.28085  19.23679  17.74943  17.17436  16.8377 ]
[  6.21556796e-316]
[ 17.4107   15.38111  14.46033  14.01726  13.66607]
[  6.21556677e-316]
[ 21.66565  19.16977  17.68089  17.11333  16.75284]
[  6.21556559e-316]
[ 21.40901  19.2438   17.76625  17.19955  16.82852]
[  6.21556440e-316]
[ 21.34932  19.16355  17.67883  17.10765  16.77806]
[  6.21556321e-316]
[ 21.57417  19.32207  17.82867  17.26183  16.92208]
[  6.21556203e-316]
[ 21.32809  19.08743  17.82183  17.30411  16.97139]
[  6.21556084e-316]
[ 21.03333  18.77113  17.362    16.82324  16.484  ]
[  6.21555966e-316]
[ 20.5206   18.72186  17.28893  16.74903  16.37328]
[  6.21555847e-316]
[ 21.78964  19.13376  17.66409  17.13135  16.76406]
[  6.21555728e-316]
[ 20.95951  19.03605  17.62573  17.12724  16.80844]
[  6.21555610e-316]
[ 20.97688  19.07124  17.55429  16.94485  16.54743]
[  6.21555491e-316]
[ 21.93806  18.89835  17.68956  17.17152  16.81151]
[  6.21555373e-316]
[ 19.54692  17.53066  16.44236  15.96543  15.58999]
[  6.21555254e-316]
[ 19.55121  17.5155   16.53911  16.1012   15.75955]
[  6.21555136e-316]
[ 21.35756  19.28337  17.78644  17.21241  16.84564]
[  6.21555017e-316]
[ 20.36967  18.49353  17.62412  17.19647  16.89283]
[  6.21554898e-316]
[ 21.09565  18.92566  17.82399  17.36692  17.01795]
[  6.21554780e-316]
[ 19.78858  17.61495  16.46447  15.97479  15.5831 ]
[  6.21554661e-316]
[ 20.03568  17.70237  16.53256  16.02192  15.633  ]
[  6.21554543e-316]
[ 19.29204  17.29615  16.19192  15.72984  15.33939]
[  6.21554424e-316]
[ 18.52076  16.47405  15.54869  15.10161  14.76246]
[  6.21554306e-316]
[ 21.21535  19.36872  17.84903  17.25583  16.87677]
[  6.21554187e-316]
[ 20.74441  18.85866  17.77716  17.33807  16.9811 ]
[  6.21554068e-316]
[ 19.7543   17.90619  16.97702  16.55806  16.2447 ]
[  6.21553950e-316]
[ 21.11246  19.06844  17.72425  17.2217   16.89004]
[  6.21553831e-316]
[ 18.95891  17.18246  16.3567   15.93764  15.63015]
[  6.21553713e-316]
[ 21.81382  19.24237  17.73512  17.19086  16.83981]
[  6.21553594e-316]
[ 17.38694  16.44198  15.93544  15.67488  15.46695]
[  6.21553476e-316]
[ 19.08245  17.31666  16.42665  16.04417  15.72076]
[  6.21553357e-316]
[ 20.12603  18.22607  16.9517   16.4528   16.09868]
[  6.21553238e-316]
[ 21.5161   19.17319  17.67944  17.11921  16.77225]
[  6.21553120e-316]
[ 19.6786   18.05117  16.9406   16.45799  16.11676]
[  6.21553001e-316]
[ 20.0585   17.97424  16.67893  16.183    15.84167]
[  6.21552883e-316]
[ 20.61777  18.78414  17.59966  17.13511  16.83641]
[  6.21552764e-316]
[ 19.66088  17.67863  16.53514  16.07375  15.69648]
[  6.21552646e-316]
[ 21.04021  18.852    17.57601  17.10304  16.72139]
[  6.21647032e-316]
[ 19.48788  17.52503  16.38646  15.91339  15.56551]
[  6.21646913e-316]
[ 17.68702  15.70894  14.86203  14.46103  14.09993]
[  6.21646795e-316]
[ 20.4832   18.63025  17.62174  17.21554  16.88907]
[  6.21646676e-316]
[ 21.46379  19.29968  17.54422  16.93969  16.58896]
[  6.21646558e-316]
[ 20.39938  18.65358  17.56367  17.13067  16.77644]
[  6.21646439e-316]
[ 20.65933  18.50073  17.15231  16.64348  16.30795]
[  6.21646320e-316]
[ 18.80564  16.92837  16.01632  15.57412  15.20936]
[  6.21646202e-316]
[ 19.90051  17.99146  16.95982  16.54522  16.20671]
[  6.21646083e-316]
[ 18.36231  16.5305   15.65423  15.26264  14.96367]
[  6.21645965e-316]
[ 19.16425  17.21648  16.16843  15.74564  15.38167]
[  6.21645846e-316]
[ 19.35936  17.42385  16.40176  15.93147  15.54756]
[  6.21645727e-316]
[ 20.00574  18.15826  16.93973  16.47584  16.11971]
[  6.21645609e-316]
[ 19.73959  17.5401   16.44063  15.97869  15.62483]
[  6.21645490e-316]
[ 21.0565   19.07498  17.71914  17.21712  16.88928]
[  6.21645372e-316]
[ 19.40944  17.61767  16.55696  16.09471  15.77065]
[  6.21645253e-316]
[ 21.03806  18.89265  17.44803  16.90027  16.54131]
[  6.21645135e-316]
[ 20.06659  18.36854  17.0548   16.49324  16.14897]
[  6.21645016e-316]
[ 18.65959  16.72513  15.73038  15.29337  14.95576]
[  6.21644897e-316]
[ 21.2844   18.88153  17.3705   16.81993  16.42155]
[  6.21644779e-316]
[ 19.16334  17.13503  16.08033  15.62021  15.2456 ]
[  6.21644660e-316]
[ 18.87984  16.99549  15.99715  15.56954  15.24985]
[  6.21644542e-316]
[ 18.02803  15.98121  15.01958  14.59115  14.27375]
[  6.21644423e-316]
[ 17.88673  16.03157  15.17102  14.79958  14.51913]
[  6.21644305e-316]
[ 20.66557  18.75823  17.62684  17.17074  16.84072]
[  6.21644186e-316]
[ 20.48644  18.51251  17.36361  16.92241  16.60254]
[  6.21644067e-316]
[ 16.79756  15.31095  14.57513  14.21711  13.92832]
[  6.21643949e-316]
[ 20.67733  18.68846  17.49356  17.00133  16.64734]
[  6.21643830e-316]
[ 20.29793  18.16933  16.94618  16.46333  16.09675]
[  6.21643712e-316]
[ 20.65475  18.78339  17.52347  17.04173  16.69723]
[  6.21643593e-316]
[ 19.10272  17.18772  16.30966  15.90497  15.59561]
[  6.21643475e-316]
[ 20.74804  18.80957  17.73875  17.3113   16.96579]
[  6.21643356e-316]
[ 19.04443  17.14919  16.17337  15.67903  15.29501]
[  6.21643237e-316]
[ 18.23248  16.49585  15.66496  15.27891  14.99565]
[  6.21643119e-316]
[ 21.93964  19.31131  17.75965  17.2461   16.90024]
[  6.21643000e-316]
[ 19.53273  17.60094  16.53397  16.12163  15.7888 ]
[  6.21642882e-316]
[ 17.09189  15.34981  14.51944  14.09949  13.787  ]
[  6.21642763e-316]
[ 17.45856  15.52805  14.59423  14.17948  13.86651]
[  6.21642645e-316]
[ 19.4642   18.07055  17.15886  16.68212  16.35703]
[  6.21642526e-316]
[ 20.53835  18.48676  17.24666  16.72318  16.41055]
[  6.21642407e-316]
[ 20.3251   18.64484  17.64279  17.23146  16.93317]
[  6.21642289e-316]
[ 20.74166  18.45943  17.12549  16.63022  16.26868]
[  6.21637071e-316]
[ 19.83716  17.93193  16.91182  16.47831  16.18455]
[  6.21636953e-316]
[ 20.57472  18.47953  17.12826  16.62259  16.29236]
[  6.21636834e-316]
[ 18.11824  16.25926  15.36627  14.95676  14.65229]
[  6.21636716e-316]
[ 16.17596  14.26905  13.38374  12.98771  12.70309]
[  6.21636597e-316]
At iteration 400 Training accuracy =  0.0025
[ 20.62924  18.38361  17.00165  16.52472  16.12907]
[  6.21636479e-316]
[ 21.17565  19.37089  17.74832  17.15819  16.82409]
[  6.21636360e-316]
[ 20.11442  18.43946  17.59922  17.18814  16.92813]
[  6.21636241e-316]
[ 17.08781  15.22046  14.40291  14.01961  13.75939]
[  6.21636123e-316]
[ 17.81709  15.92922  15.0768   14.69021  14.39318]
[  6.21636004e-316]
[ 20.87791  18.60716  17.45442  16.99169  16.63344]
[  6.21635886e-316]
[ 19.21787  17.18116  16.1534   15.71939  15.36253]
[  6.21635767e-316]
[ 18.54327  16.56232  15.65547  15.23033  14.89937]
[  6.21635649e-316]
[ 18.18956  16.30441  15.42194  14.99433  14.66555]
[  6.21635530e-316]
[ 19.15939  17.40947  16.56433  16.19003  15.90783]
[  6.21635411e-316]
[ 20.47064  18.36409  16.89847  16.38074  16.02514]
[  6.21635293e-316]
[ 19.27387  17.30969  16.31619  15.88062  15.5352 ]
[  6.21635174e-316]
[ 18.91801  16.90807  15.87845  15.38591  15.00047]
[  6.21635056e-316]
[ 18.27822  16.22331  15.2249   14.79706  14.46722]
[  6.21634937e-316]
[ 19.36811  17.25236  16.14813  15.68917  15.30785]
[  6.21634819e-316]
[ 19.36072  17.48932  16.39222  15.95395  15.60998]
[  6.21634700e-316]
[ 19.72808  18.21461  17.29881  16.90913  16.61112]
[  6.21634581e-316]
[ 20.35837  18.35098  17.09641  16.61169  16.23149]
[  6.21634463e-316]
[ 19.56821  17.60756  16.39931  15.86242  15.41191]
[  6.21634344e-316]
[ 17.90874  15.95084  15.03342  14.60443  14.2768 ]
[  6.21634226e-316]
[ 20.60093  18.67151  17.50318  17.03002  16.65414]
[  6.21634107e-316]
[ 18.97243  17.03223  16.01145  15.56209  15.2159 ]
[  6.21633988e-316]
[ 19.14682  17.04302  16.02793  15.5843   15.21697]
[  6.21633870e-316]
[ 19.83676  18.42266  17.52122  17.09325  16.80614]
[  6.21633751e-316]
[ 20.80095  18.73201  17.6197   17.12504  16.7464 ]
[  6.21633633e-316]

>>> ================================ RESTART ================================
>>> 
[ 19.20599  17.04256  15.99127  15.51565  15.12117]
(10000L, 5L)
Time before training =  1439633098.19
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  172.338000059 seconds
At iteration 100 Training accuracy =  1.01
At iteration 200 Training accuracy =  1.005
At iteration 300 Training accuracy =  1.00333333333
At iteration 400 Training accuracy =  1.0025
At iteration 500 Training accuracy =  1.002
At iteration 600 Training accuracy =  1.00166666667
At iteration 700 Training accuracy =  1.00142857143
At iteration 800 Training accuracy =  1.00125
At iteration 900 Training accuracy =  1.00111111111
At iteration 1000 Training accuracy =  1.001
At iteration 1100 Training accuracy =  1.00090909091
At iteration 1200 Training accuracy =  1.00083333333
At iteration 1300 Training accuracy =  1.00076923077
At iteration 1400 Training accuracy =  1.00071428571
At iteration 1500 Training accuracy =  1.00066666667
At iteration 1600 Training accuracy =  1.000625
At iteration 1700 Training accuracy =  1.00058823529
At iteration 1800 Training accuracy =  1.00055555556
At iteration 1900 Training accuracy =  1.00052631579
At iteration 2000 Training accuracy =  1.0005
At iteration 2100 Training accuracy =  1.00047619048
At iteration 2200 Training accuracy =  1.00045454545
At iteration 2300 Training accuracy =  1.00043478261
At iteration 2400 Training accuracy =  1.00041666667
At iteration 2500 Training accuracy =  1.0004
At iteration 2600 Training accuracy =  1.00038461538
At iteration 2700 Training accuracy =  1.00037037037
At iteration 2800 Training accuracy =  1.00035714286
At iteration 2900 Training accuracy =  1.00034482759
At iteration 3000 Training accuracy =  1.00033333333
At iteration 3100 Training accuracy =  1.00032258065
At iteration 3200 Training accuracy =  1.0003125
At iteration 3300 Training accuracy =  1.0003030303
At iteration 3400 Training accuracy =  1.00029411765
At iteration 3500 Training accuracy =  1.00028571429
At iteration 3600 Training accuracy =  1.00027777778
At iteration 3700 Training accuracy =  1.00027027027
At iteration 3800 Training accuracy =  1.00026315789
At iteration 3900 Training accuracy =  1.00025641026
At iteration 4000 Training accuracy =  1.00025
At iteration 4100 Training accuracy =  1.00024390244
At iteration 4200 Training accuracy =  1.00023809524
At iteration 4300 Training accuracy =  1.00023255814
At iteration 4400 Training accuracy =  1.00022727273
At iteration 4500 Training accuracy =  1.00022222222
At iteration 4600 Training accuracy =  1.0002173913
At iteration 4700 Training accuracy =  1.00021276596
At iteration 4800 Training accuracy =  1.00020833333
At iteration 4900 Training accuracy =  1.00020408163
Training accuracy =  1.0
Time =  5900.79399991 seconds

Testing accuracy =  0.0004
Time =  11141.2579999 seconds
>>> ================================ RESTART ================================
>>> 

Traceback (most recent call last):
  File "C:/Users/S/Documents/Sonu/SciComp/Research/AstroP1/ap1_sklearn_svm2.py", line 29, in <module>
    trainingSetLabels = trainingSet[:,7]  #putting labels in separate array
TypeError: 'NoneType' object has no attribute '__getitem__'
>>> ================================ RESTART ================================
>>> 

Traceback (most recent call last):
  File "C:/Users/S/Documents/Sonu/SciComp/Research/AstroP1/ap1_sklearn_svm2.py", line 29, in <module>
    trainingSetLabels = trainingSet[:,7:7]  #putting labels in separate array
TypeError: 'NoneType' object has no attribute '__getitem__'
>>> ================================ RESTART ================================
>>> 
Time before training =  1439697504.85
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  0.0429999828339 seconds

Traceback (most recent call last):
  File "C:/Users/S/Documents/Sonu/SciComp/Research/AstroP1/ap1_sklearn_svm2.py", line 44, in <module>
    trainingAccuracy = score(trainingSet, trainingSetLabels)
NameError: name 'score' is not defined
>>> ================================ RESTART ================================
>>> 
Time before training =  1439697546.72
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  0.050999879837 seconds
Training accuracy =  0.84
Time =  0.0729999542236 seconds

Testing accuracy =  0.851
Time =  0.0969998836517 seconds
>>> ================================ RESTART ================================
>>> 
Time before training =  1439697628.66
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  1.9279999733 seconds
Training accuracy =  0.8726
Time =  2.76699995995 seconds

Testing accuracy =  0.8774
Time =  3.61599993706 seconds
>>> ================================ RESTART ================================
>>> 
Time before training =  1439697670.78
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  147.373999834 seconds
Training accuracy =  0.88904
Time =  215.712999821 seconds

Testing accuracy =  0.88252846866
Time =  261.346999884 seconds
>>> ================================ RESTART ================================
>>> 
Time before training =  1439700763.77
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Done training! Time =  149.174999952 seconds
Training accuracy =  0.88903
Time =  217.405999899 seconds

Testing accuracy =  0.88252846866
Time =  258.842999935 seconds
>>> len(ellipticals)
61998
>>> len(spirals)
189813
>>> len(uncertains)
413960
>>> len(ellipticals)+len(uncertains)+len(spirals)
665771
>>> len(trainingSetEllipticals)
50000
>>> len(testingSetEllipticals)
11998
>>> len(testingSetSpirals)
50000
>>> classificationSet = uncertains[:, 1:6]
>>> predictions = clf.predict(classificationSet)
>>> print predictions[:10]
[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]
>>> print len(predictions[predictions==1])
137297
>>> print len(predictions[predictions==-1])
276663
>>> print 137297+276663
413960
>>> 
